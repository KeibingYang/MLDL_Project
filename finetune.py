"""
Florenceæ¨¡å‹åœ¨DocVQAæ•°æ®é›†ä¸Šçš„å¤šå±‚æ¬¡å¾®è°ƒè®­ç»ƒè„šæœ¬
=======================================================

æ”¯æŒå››ä¸ªè®­ç»ƒå±‚æ¬¡çš„è§†è§‰ç¼–ç å™¨å†»ç»“/è§£å†»ç­–ç•¥ï¼š
1. å…¨éƒ¨å†»ç»“ (0% è§†è§‰ç¼–ç å™¨å¯è®­ç»ƒ)
2. è§£å†»1/3 (33% è§†è§‰ç¼–ç å™¨å¯è®­ç»ƒ)  
3. è§£å†»2/3 (66% è§†è§‰ç¼–ç å™¨å¯è®­ç»ƒ)
4. å…¨éƒ¨è§£å†» (100% è§†è§‰ç¼–ç å™¨å¯è®­ç»ƒ)

æ¯ä¸ªå±‚æ¬¡åŒ…å«ï¼š
- è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—è®°å½•
- éªŒè¯é›†æ€§èƒ½è¯„ä¼°
- é¢„æµ‹ç»“æœå¯è§†åŒ–
- ç»Ÿè®¡æŒ‡æ ‡è®¡ç®—

ç¬¦åˆç§‘ç ”è§„èŒƒï¼ŒåŒ…å«å®Œæ•´çš„å®éªŒè®°å½•å’Œåˆ†æ
"""

from datasets import load_dataset
from PIL import Image
import os
from tqdm import tqdm
import torch
import matplotlib.pyplot as plt
from transformers import AutoProcessor, AutoModelForCausalLM
from torch.utils.data import Dataset, DataLoader
from transformers import AdamW, get_scheduler
from rapidfuzz.distance import Levenshtein
import shutil
import numpy as np
import json
import time
from datetime import datetime


def compute_levenshtein_similarity(preds, refs):
    """
    è®¡ç®—é¢„æµ‹ç»“æœä¸å‚è€ƒç­”æ¡ˆä¹‹é—´çš„Levenshteinç›¸ä¼¼åº¦
    
    Args:
        preds (list[str] or str): é¢„æµ‹ç­”æ¡ˆ
        refs (list[str] or str): å‚è€ƒç­”æ¡ˆ
    
    Returns:
        tuple: (ç›¸ä¼¼åº¦æ•°ç»„, å¹³å‡ç›¸ä¼¼åº¦)
    """
    if isinstance(preds, str):
        preds = [preds]
    if isinstance(refs, str):
        refs = [refs]
    
    sims = []
    for p, r in zip(preds, refs):
        # æ–‡æœ¬é¢„å¤„ç†
        p_norm = str(p).strip().lower()
        r_norm = str(r).strip().lower()
        
        # è®¡ç®—Levenshteinè·ç¦»
        dist = Levenshtein.distance(p_norm, r_norm)
        
        # å½’ä¸€åŒ–ä¸ºç›¸ä¼¼åº¦
        denom = max(len(p_norm), len(r_norm)) or 1
        sims.append(1 - dist / denom)
    
    sims = np.round(np.array(sims), 4)
    return sims, float(np.round(sims.mean(), 4))


def set_vision_encoder_trainable_ratio(model, ratio):
    """
    æ ¹æ®ratioè®¾ç½®è§†è§‰ç¼–ç å™¨çš„å¯è®­ç»ƒæ¯”ä¾‹
    
    Args:
        model: Florenceæ¨¡å‹å®ä¾‹
        ratio (float): å¯è®­ç»ƒæ¯”ä¾‹ (0.0=å…¨éƒ¨å†»ç»“, 1.0=å…¨éƒ¨è§£å†»)
    
    Returns:
        dict: åŒ…å«è§†è§‰ç¼–ç å™¨å‚æ•°ç»Ÿè®¡ä¿¡æ¯
    """
    vision_params = []
    for name, param in model.named_parameters():
        if 'vision_tower' in name:
            vision_params.append(param)
    
    total_vision_params = len(vision_params)
    num_to_unfreeze = int(total_vision_params * ratio)
    
    # é¦–å…ˆå†»ç»“æ‰€æœ‰è§†è§‰ç¼–ç å™¨å‚æ•°
    for param in vision_params:
        param.requires_grad = False
    
    # è§£å†»æŒ‡å®šæ¯”ä¾‹çš„å‚æ•°ï¼ˆä»åå¾€å‰ï¼Œé€šå¸¸åé¢çš„å±‚æ›´é‡è¦ï¼‰
    if num_to_unfreeze > 0:
        for param in vision_params[-num_to_unfreeze:]:
            param.requires_grad = True
    
    info = {
        'total_vision_params': total_vision_params,
        'unfrozen_vision_params': num_to_unfreeze,
        'vision_trainable_ratio': ratio
    }
    
    print(f"ğŸ¯ Vision encoder trainable ratio set to {ratio:.2f}")
    print(f"   Unfrozen {num_to_unfreeze} of {total_vision_params} vision parameters")
    
    return info


def get_trainable_params_info(model):
    """
    è·å–æ¨¡å‹è¯¦ç»†çš„å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    
    Args:
        model: æ¨¡å‹å®ä¾‹
    
    Returns:
        dict: è¯¦ç»†çš„å‚æ•°ç»Ÿè®¡ä¿¡æ¯
    """
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    vision_params = sum(p.numel() for name, p in model.named_parameters() if 'vision_tower' in name)
    vision_trainable = sum(p.numel() for name, p in model.named_parameters() 
                          if 'vision_tower' in name and p.requires_grad)
    language_params = total_params - vision_params
    language_trainable = trainable_params - vision_trainable
    
    info = {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'frozen_params': total_params - trainable_params,
        'trainable_ratio': trainable_params / total_params,
        'vision_params': vision_params,
        'vision_trainable': vision_trainable,
        'vision_trainable_ratio': vision_trainable / vision_params if vision_params > 0 else 0,
        'language_params': language_params,
        'language_trainable': language_trainable,
        'language_trainable_ratio': language_trainable / language_params if language_params > 0 else 0
    }
    
    print(f"ğŸ“Š Model Parameters Info:")
    print(f"   Total: {total_params:,}")
    print(f"   Trainable: {trainable_params:,} ({info['trainable_ratio']:.2%})")
    print(f"   Frozen: {info['frozen_params']:,}")
    print(f"   Vision Total: {vision_params:,}")
    print(f"   Vision Trainable: {vision_trainable:,} ({info['vision_trainable_ratio']:.2%})")
    print(f"   Language Total: {language_params:,}")
    print(f"   Language Trainable: {language_trainable:,} ({info['language_trainable_ratio']:.2%})")
    
    return info


def compute_em_f1(preds, refs):
    """
    è®¡ç®—EM (Exact Match) å’Œ F1 åˆ†æ•°
    
    Args:
        preds (list[str]): é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨
        refs (list[str]): å‚è€ƒç­”æ¡ˆåˆ—è¡¨
    
    Returns:
        tuple: (EMåˆ†æ•°, F1åˆ†æ•°)
    """
    try:
        from evaluate import load
        squad_metric = load("squad")
        
        formatted_preds = [{"id": str(i), "prediction_text": str(p)} for i, p in enumerate(preds)]
        formatted_refs = [{"id": str(i), "answers": {"text": [str(a)], "answer_start": [0]}} for i, a in enumerate(refs)]
        
        results = squad_metric.compute(predictions=formatted_preds, references=formatted_refs)
        return results["exact_match"], results["f1"]
    except Exception as e:
        print(f"Warning: Could not compute EM/F1 using evaluate library: {e}")
        # ç®€å•çš„EMè®¡ç®—ä½œä¸ºå¤‡é€‰
        em_count = sum(1 for p, r in zip(preds, refs) if str(p).strip().lower() == str(r).strip().lower())
        em_score = em_count / len(preds) * 100
        return em_score, em_score


def train_model_with_logging(train_loader, val_loader, model, processor, device, epochs=2, lr=1e-6, 
                             vision_trainable_ratio=0.0, log_file_path="train_log.txt", stage_name=""):
    """
    è®­ç»ƒæ¨¡å‹å¹¶è®°å½•è¯¦ç»†æ—¥å¿—
    
    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        model: æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨
        device: è®¡ç®—è®¾å¤‡
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        vision_trainable_ratio: è§†è§‰ç¼–ç å™¨å¯è®­ç»ƒæ¯”ä¾‹
        log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        stage_name: è®­ç»ƒé˜¶æ®µåç§°
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ Starting Training Stage: {stage_name}")
    print(f"{'='*60}")
    
    # è®¾ç½®è®­ç»ƒå¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è®¾ç½®è§†è§‰ç¼–ç å™¨å†»ç»“æ¯”ä¾‹
    vision_info = set_vision_encoder_trainable_ratio(model, vision_trainable_ratio)
    
    # è·å–å¹¶æ˜¾ç¤ºå‚æ•°ä¿¡æ¯
    params_info = get_trainable_params_info(model)
    
    # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = AdamW(trainable_params, lr=lr)
    
    num_training_steps = epochs * len(train_loader)
    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps,
    )
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = open(log_file_path, "w", encoding='utf-8')
    
    # è®°å½•è®­ç»ƒé…ç½®
    config_info = {
        "timestamp": datetime.now().isoformat(),
        "stage_name": stage_name,
        "vision_trainable_ratio": vision_trainable_ratio,
        "learning_rate": lr,
        "epochs": epochs,
        "batch_size": train_loader.batch_size,
        "device": str(device),
        "params_info": params_info,
        "vision_info": vision_info
    }
    
    log_file.write("=" * 60 + "\n")
    log_file.write(f"Training Stage: {stage_name}\n")
    log_file.write(f"Timestamp: {config_info['timestamp']}\n")
    log_file.write("=" * 60 + "\n")
    log_file.write(f"Vision Trainable Ratio: {vision_trainable_ratio:.2f}\n")
    log_file.write(f"Total Parameters: {params_info['total_params']:,}\n")
    log_file.write(f"Trainable Parameters: {params_info['trainable_params']:,} ({params_info['trainable_ratio']:.2%})\n")
    log_file.write(f"Vision Parameters: {params_info['vision_params']:,}\n")
    log_file.write(f"Vision Trainable: {params_info['vision_trainable']:,} ({params_info['vision_trainable_ratio']:.2%})\n")
    log_file.write(f"Language Parameters: {params_info['language_params']:,}\n")
    log_file.write(f"Language Trainable: {params_info['language_trainable']:,} ({params_info['language_trainable_ratio']:.2%})\n")
    log_file.write(f"Learning Rate: {lr}\n")
    log_file.write(f"Epochs: {epochs}\n")
    log_file.write(f"Batch Size: {train_loader.batch_size}\n")
    log_file.write(f"Device: {device}\n")
    log_file.write("-" * 50 + "\n")
    
    # ä¿å­˜é…ç½®ä¸ºJSON
    config_file = log_file_path.replace('.txt', '_config.json')
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config_info, f, indent=2, ensure_ascii=False)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        num_batches = 0
        
        epoch_start_time = time.time()
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{epochs}")):
            inputs, answers = batch
            input_ids = inputs["input_ids"]
            pixel_values = inputs["pixel_values"]
            
            # å¤„ç†ç­”æ¡ˆæ ¼å¼
            processed_answers = []
            for ans in answers:
                if isinstance(ans, list):
                    processed_answers.append(ans[0] if ans else "")
                else:
                    processed_answers.append(str(ans))
            
            labels = processor.tokenizer(
                text=processed_answers, 
                return_tensors="pt", 
                padding=True, 
                return_token_type_ids=False
            ).input_ids.to(device)
            
            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
            loss = outputs.loss
            
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            
            train_loss += loss.item()
            num_batches += 1
        
        avg_train_loss = train_loss / num_batches
        epoch_train_time = time.time() - epoch_start_time
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        predictions = []
        ground_truths = []
        val_batches = 0
        
        val_start_time = time.time()
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Validation Epoch {epoch + 1}/{epochs}"):
                inputs, answers = batch
                input_ids = inputs["input_ids"]
                pixel_values = inputs["pixel_values"]
                
                # å¤„ç†ç­”æ¡ˆæ ¼å¼
                processed_answers = []
                for ans in answers:
                    if isinstance(ans, list):
                        processed_answers.append(ans[0] if ans else "")
                    else:
                        processed_answers.append(str(ans))
                
                labels = processor.tokenizer(
                    text=processed_answers, 
                    return_tensors="pt", 
                    padding=True, 
                    return_token_type_ids=False
                ).input_ids.to(device)
                
                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
                val_loss += outputs.loss.item()
                val_batches += 1
                
                # ç”Ÿæˆé¢„æµ‹ç­”æ¡ˆ
                gen_ids = model.generate(
                    input_ids=input_ids, 
                    pixel_values=pixel_values, 
                    max_new_tokens=128,
                    num_beams=3
                )
                gen_texts = processor.batch_decode(gen_ids, skip_special_tokens=True)
                
                predictions.extend(gen_texts)
                ground_truths.extend(processed_answers)
        
        val_time = time.time() - val_start_time
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        avg_val_loss = val_loss / val_batches
        em, f1 = compute_em_f1(predictions, ground_truths)
        _, avg_similarity = compute_levenshtein_similarity(predictions, ground_truths)
        
        # è®°å½•epochç»“æœ
        epoch_info = {
            "epoch": epoch + 1,
            "train_loss": avg_train_loss,
            "val_loss": avg_val_loss,
            "em": em,
            "f1": f1,
            "avg_similarity": avg_similarity,
            "train_time": epoch_train_time,
            "val_time": val_time
        }
        
        # æ‰“å°å’Œå†™å…¥æ—¥å¿—
        print(f"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, EM: {em:.2f}, F1: {f1:.2f}, Avg Sim: {avg_similarity:.4f}")
        log_file.write(f"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, EM: {em:.2f}, F1: {f1:.2f}, Avg Sim: {avg_similarity:.4f}\n")
        log_file.write(f"           Train Time: {epoch_train_time:.2f}s, Val Time: {val_time:.2f}s\n")
        
        # ä¿å­˜epochè¯¦ç»†ä¿¡æ¯
        epoch_file = log_file_path.replace('.txt', f'_epoch_{epoch+1}.json')
        with open(epoch_file, 'w', encoding='utf-8') as f:
            json.dump(epoch_info, f, indent=2, ensure_ascii=False)
    
    total_time = time.time() - start_time
    log_file.write(f"\nTotal Training Time: {total_time:.2f}s\n")
    log_file.write("=" * 60 + "\n")
    log_file.close()


def visualize_and_log(dataset, model, processor, device, num_samples=100, save_dir="./visualization", stage_name=""):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœå¹¶è®°å½•è¯¦ç»†æ•°æ®
    
    Args:
        dataset: æ•°æ®é›†
        model: æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨
        device: è®¡ç®—è®¾å¤‡
        num_samples: å¯è§†åŒ–æ ·æœ¬æ•°é‡
        save_dir: ä¿å­˜ç›®å½•
        stage_name: é˜¶æ®µåç§°
    
    Returns:
        tuple: (å‡†ç¡®ç‡, å¹³å‡ç›¸ä¼¼åº¦)
    """
    print(f"\nğŸ“Š Starting Visualization for {stage_name}")
    
    model.eval()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(os.path.join(save_dir, "images"), exist_ok=True)
    
    # åˆå§‹åŒ–è®¡æ•°å™¨å’Œè®°å½•
    correct_count = 0
    total_similarity = 0
    prediction_details = []
    
    log_file_path = os.path.join(save_dir, f"predictions_log_{stage_name}.txt")
    log_file = open(log_file_path, "w", encoding='utf-8')
    
    # å†™å…¥å¤´éƒ¨ä¿¡æ¯
    log_file.write("=" * 60 + "\n")
    log_file.write(f"Prediction Results for {stage_name}\n")
    log_file.write(f"Timestamp: {datetime.now().isoformat()}\n")
    log_file.write(f"Total Samples: {min(num_samples, len(dataset))}\n")
    log_file.write("=" * 60 + "\n")
    
    results_summary = []
    
    for idx in tqdm(range(min(num_samples, len(dataset))), desc=f"Visualizing {stage_name}"):
        try:
            question, gt_answer, image = dataset[idx]
            
            # å¤„ç†ç­”æ¡ˆæ ¼å¼
            if isinstance(gt_answer, list):
                gt_answer = gt_answer[0] if gt_answer else ""
            gt_answer = str(gt_answer)
            
            # ä¿å­˜å›¾åƒ
            image_path = os.path.join(save_dir, "images", f"image_{stage_name}_{idx}.png")
            image.save(image_path)
            
            # è¿›è¡Œæ¨ç†
            prompt = question
            if image.mode != "RGB":
                image = image.convert("RGB")
            
            inputs = processor(text=prompt, images=image, return_tensors="pt").to(device)
            
            with torch.no_grad():
                generated_ids = model.generate(
                    input_ids=inputs["input_ids"],
                    pixel_values=inputs["pixel_values"],
                    max_new_tokens=128,
                    num_beams=3
                )
            
            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
            generated_text = generated_texts[0] if generated_texts else ""
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            _, similarity = compute_levenshtein_similarity([generated_text], [gt_answer])
            
            # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
            is_correct = gt_answer.strip().lower() in generated_text.strip().lower()
            if is_correct:
                correct_count += 1
            
            total_similarity += similarity
            
            # è®°å½•è¯¦ç»†ä¿¡æ¯
            result_info = {
                'idx': idx,
                'question': question,
                'gt_answer': gt_answer,
                'prediction': generated_text,
                'similarity': similarity,
                'is_correct': is_correct,
                'image_path': image_path
            }
            results_summary.append(result_info)
            prediction_details.append(result_info)
            
            # å†™å…¥æ—¥å¿—
            log_file.write(f"\n--- Example {idx+1} ---\n")
            log_file.write(f"Question: {question}\n")
            log_file.write(f"GT Answer: {gt_answer}\n")
            log_file.write(f"Predicted: {generated_text}\n")
            log_file.write(f"Similarity: {similarity:.4f}\n")
            log_file.write(f"Status: {'âœ” Correct' if is_correct else 'âœ˜ Incorrect'}\n")
            
        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            log_file.write(f"\n--- Example {idx+1} ---\n")
            log_file.write(f"ERROR: {str(e)}\n")
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    num_processed = min(num_samples, len(dataset))
    accuracy = correct_count / num_processed if num_processed > 0 else 0
    avg_similarity = total_similarity / num_processed if num_processed > 0 else 0
    
    # åˆ›å»ºç»Ÿè®¡æ±‡æ€»
    summary_stats = {
        "stage_name": stage_name,
        "timestamp": datetime.now().isoformat(),
        "total_samples": num_processed,
        "correct_predictions": correct_count,
        "accuracy": accuracy,
        "average_similarity": avg_similarity,
        "metrics": {
            "accuracy_percentage": accuracy * 100,
            "error_rate": (1 - accuracy) * 100,
            "similarity_std": np.std([r['similarity'] for r in prediction_details]) if prediction_details else 0
        }
    }
    
    summary_text = f"""
Evaluation Summary for {stage_name}:
==========================================
Total Samples: {num_processed}
Correct Predictions: {correct_count}
Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)
Average Similarity: {avg_similarity:.4f}
Similarity Std: {summary_stats['metrics']['similarity_std']:.4f}
Error Rate: {summary_stats['metrics']['error_rate']:.2f}%
"""
    
    # ä¿å­˜æ±‡æ€»ä¿¡æ¯
    summary_path = os.path.join(save_dir, f"summary_{stage_name}.txt")
    with open(summary_path, "w", encoding='utf-8') as f:
        f.write(summary_text)
    
    # ä¿å­˜è¯¦ç»†JSONæ•°æ®
    details_path = os.path.join(save_dir, f"details_{stage_name}.json")
    with open(details_path, 'w', encoding='utf-8') as f:
        json.dump({
            "summary": summary_stats,
            "predictions": prediction_details
        }, f, indent=2, ensure_ascii=False)
    
    print(summary_text)
    log_file.write(summary_text)
    log_file.close()
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    create_visualization_grids(results_summary, save_dir, stage_name)
    create_metrics_charts(prediction_details, save_dir, stage_name)
    
    return accuracy, avg_similarity


def create_visualization_grids(results_summary, save_dir, stage_name):
    """
    åˆ›å»ºé¢„æµ‹ç»“æœçš„å¯è§†åŒ–ç½‘æ ¼å›¾
    
    Args:
        results_summary: ç»“æœæ±‡æ€»åˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
        stage_name: é˜¶æ®µåç§°
    """
    def create_single_grid(results_batch, save_dir, batch_idx):
        """åˆ›å»ºå•ä¸ªç½‘æ ¼å›¾"""
        fig, axes = plt.subplots(2, 5, figsize=(25, 10))
        fig.suptitle(f'Predictions Visualization - {stage_name} (Batch {batch_idx + 1})', fontsize=16, fontweight='bold')
        
        for i, result in enumerate(results_batch):
            if i >= 10:
                break
            
            row = i // 5
            col = i % 5
            
            try:
                # åŠ è½½å’Œæ˜¾ç¤ºå›¾åƒ
                img = Image.open(result['image_path'])
                axes[row, col].imshow(img)
                axes[row, col].axis('off')
                
                # åˆ›å»ºæ ‡é¢˜
                status_color = 'green' if result['is_correct'] else 'red'
                status_symbol = 'âœ“' if result['is_correct'] else 'âœ—'
                
                title = f"{status_symbol} Sim: {result['similarity']:.3f}\n"
                
                # æˆªæ–­é•¿æ–‡æœ¬
                gt_text = result['gt_answer'][:25] + "..." if len(result['gt_answer']) > 25 else result['gt_answer']
                pred_text = result['prediction'][:25] + "..." if len(result['prediction']) > 25 else result['prediction']
                
                title += f"GT: {gt_text}\n"
                title += f"Pred: {pred_text}"
                
                axes[row, col].set_title(title, fontsize=10, color=status_color, fontweight='bold')
                
            except Exception as e:
                axes[row, col].text(0.5, 0.5, f"Error loading\nimage {result['idx']}\n{str(e)}", 
                                  ha='center', va='center', transform=axes[row, col].transAxes,
                                  fontsize=10, color='red')
                axes[row, col].axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(results_batch), 10):
            row = i // 5
            col = i % 5
            axes[row, col].axis('off')
        
        plt.tight_layout()
        save_path = os.path.join(save_dir, f"visualization_{stage_name}_batch_{batch_idx + 1}.png")
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    # åˆ†æ‰¹æ¬¡åˆ›å»ºå¯è§†åŒ–å›¾
    batch_size = 10
    for batch_idx in range((len(results_summary) + batch_size - 1) // batch_size):
        batch_results = results_summary[batch_idx * batch_size:(batch_idx + 1) * batch_size]
        if batch_results:
            create_single_grid(batch_results, save_dir, batch_idx)


def create_metrics_charts(prediction_details, save_dir, stage_name):
    """
    åˆ›å»ºæŒ‡æ ‡ç»Ÿè®¡å›¾è¡¨
    
    Args:
        prediction_details: é¢„æµ‹è¯¦ç»†ä¿¡æ¯
        save_dir: ä¿å­˜ç›®å½•
        stage_name: é˜¶æ®µåç§°
    """
    if not prediction_details:
        return
    
    # æå–æ•°æ®
    similarities = [p['similarity'] for p in prediction_details]
    accuracies = [1 if p['is_correct'] else 0 for p in prediction_details]
    
    # åˆ›å»ºå¤šå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'Metrics Analysis - {stage_name}', fontsize=16, fontweight='bold')
    
    # ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾
    axes[0, 0].hist(similarities, bins=20, alpha=0.7, color='blue', edgecolor='black')
    axes[0, 0].set_title('Similarity Distribution')
    axes[0, 0].set_xlabel('Similarity Score')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].axvline(np.mean(similarities), color='red', linestyle='--', label=f'Mean: {np.mean(similarities):.3f}')
    axes[0, 0].legend()
    
    # å‡†ç¡®ç‡è¶‹åŠ¿
    window_size = max(1, len(accuracies) // 20)
    rolling_acc = np.convolve(accuracies, np.ones(window_size)/window_size, mode='valid')
    axes[0, 1].plot(rolling_acc, color='green', linewidth=2)
    axes[0, 1].set_title(f'Accuracy Trend (Rolling Window: {window_size})')
    axes[0, 1].set_xlabel('Sample Index')
    axes[0, 1].set_ylabel('Rolling Accuracy')
    axes[0, 1].set_ylim(0, 1)
    
    # ç›¸ä¼¼åº¦ vs å‡†ç¡®ç‡æ•£ç‚¹å›¾
    colors = ['red' if acc == 0 else 'green' for acc in accuracies]
    axes[1, 0].scatter(similarities, accuracies, c=colors, alpha=0.6)
    axes[1, 0].set_title('Similarity vs Accuracy')
    axes[1, 0].set_xlabel('Similarity Score')
    axes[1, 0].set_ylabel('Accuracy (0/1)')
    axes[1, 0].set_ylim(-0.1, 1.1)
    
    # ç»Ÿè®¡æ±‡æ€»
    stats_text = f"""
Statistics Summary:
Total Samples: {len(prediction_details)}
Correct: {sum(accuracies)}
Accuracy: {np.mean(accuracies):.3f}
Mean Similarity: {np.mean(similarities):.3f}
Std Similarity: {np.std(similarities):.3f}
Min Similarity: {np.min(similarities):.3f}
Max Similarity: {np.max(similarities):.3f}
"""
    
    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, 
                    fontsize=12, verticalalignment='top', fontfamily='monospace')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, f"metrics_analysis_{stage_name}.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def run_all_training_stages(train_loader, val_loader, model, processor, device, epochs=2, lr=1e-6, num_samples=100):
    """
    è¿è¡Œæ‰€æœ‰å››ä¸ªè®­ç»ƒé˜¶æ®µ
    
    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        model: æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨
        device: è®¡ç®—è®¾å¤‡
        epochs: æ¯ä¸ªé˜¶æ®µçš„è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        num_samples: å¯è§†åŒ–æ ·æœ¬æ•°é‡
    
    Returns:
        dict: æ‰€æœ‰é˜¶æ®µçš„ç»“æœæ±‡æ€»
    """
    # å®šä¹‰å››ä¸ªå†»ç»“å±‚æ¬¡
    freezing_levels = [0, 0.33, 0.66, 1.0]
    stage_names = ["all_frozen", "one_third_unfrozen", "two_thirds_unfrozen", "all_unfrozen"]

    results = {}
    overall_start_time = time.time()
    
    print(f"\nğŸš€ Starting Multi-Stage Training Pipeline")
    print(f"   Stages: {len(stage_names)}")
    print(f"   Epochs per stage: {epochs}")
    print(f"   Learning rate: {lr}")
    print(f"   Visualization samples: {num_samples}")
    
    for stage_idx, (ratio, stage) in enumerate(zip(freezing_levels, stage_names)):
        stage_start_time = time.time()
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Stage {stage_idx + 1}/4: {stage} (Vision Trainable: {ratio:.0%})")
        print(f"{'='*80}")
        
        # è®­ç»ƒé˜¶æ®µ
        log_file = f"train_log_{stage}.txt"
        train_model_with_logging(
            train_loader=train_loader,
            val_loader=val_loader,
            model=model,
            processor=processor,
            device=device,
            epochs=epochs,
            lr=lr,
            vision_trainable_ratio=ratio,
            log_file_path=log_file,
            stage_name=stage
        )
        
        # å¯è§†åŒ–å’Œè¯„ä¼°é˜¶æ®µ
        save_dir = f"./visualization_{stage}"
        acc, sim = visualize_and_log(
            dataset=val_loader.dataset,
            model=model,
            processor=processor,
            device=device,
            num_samples=num_samples,
            save_dir=save_dir,
            stage_name=stage
        )
        
        stage_time = time.time() - stage_start_time
        
        # è®°å½•ç»“æœ
        results[stage] = {
            'accuracy': acc,
            'similarity': sim,
            'vision_trainable_ratio': ratio,
            'stage_time': stage_time,
            'stage_index': stage_idx + 1
        }
        
        print(f"âœ… Stage {stage} completed in {stage_time:.2f}s")
        print(f"   Accuracy: {acc:.4f}, Similarity: {sim:.4f}")
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        intermediate_results_path = f"intermediate_results_{stage}.json"
        with open(intermediate_results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæ€»ä½“ç»“æœæ±‡æ€»
    total_time = time.time() - overall_start_time
    final_results = {
        "pipeline_info": {
            "total_time": total_time,
            "num_stages": len(stage_names),
            "epochs_per_stage": epochs,
            "learning_rate": lr,
            "num_visualization_samples": num_samples,
            "completion_time": datetime.now().isoformat()
        },
        "stage_results": results
    }
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_path = "final_results_all_stages.json"
    with open(final_results_path, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    create_comparison_charts(results, "./")
    
    print(f"\nğŸ‰ All training stages completed in {total_time:.2f}s")
    print(f"ğŸ“Š Results saved to: {final_results_path}")
    
    return results


def create_comparison_charts(results, save_dir):
    """
    åˆ›å»ºå„é˜¶æ®µå¯¹æ¯”å›¾è¡¨
    
    Args:
        results: æ‰€æœ‰é˜¶æ®µçš„ç»“æœ
        save_dir: ä¿å­˜ç›®å½•
    """
    stage_names = list(results.keys())
    accuracies = [results[stage]['accuracy'] for stage in stage_names]
    similarities = [results[stage]['similarity'] for stage in stage_names]
    ratios = [results[stage]['vision_trainable_ratio'] for stage in stage_names]
    
    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Multi-Stage Training Comparison', fontsize=16, fontweight='bold')
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    axes[0, 0].bar(stage_names, accuracies, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('Accuracy Comparison')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_ylim(0, max(accuracies) * 1.1)
    for i, v in enumerate(accuracies):
        axes[0, 0].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')
    
    # ç›¸ä¼¼åº¦å¯¹æ¯”
    axes[0, 1].bar(stage_names, similarities, color='lightgreen', alpha=0.7)
    axes[0, 1].set_title('Similarity Comparison')
    axes[0, 1].set_ylabel('Average Similarity')
    axes[0, 1].set_ylim(0, max(similarities) * 1.1)
    for i, v in enumerate(similarities):
        axes[0, 1].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')
    
    # æ€§èƒ½è¶‹åŠ¿
    axes[1, 0].plot(ratios, accuracies, 'bo-', label='Accuracy', linewidth=2, markersize=8)
    axes[1, 0].plot(ratios, similarities, 'ro-', label='Similarity', linewidth=2, markersize=8)
    axes[1, 0].set_title('Performance vs Vision Trainable Ratio')
    axes[1, 0].set_xlabel('Vision Trainable Ratio')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # æ”¹è¿›ç»Ÿè®¡
    acc_improvements = [accuracies[i] - accuracies[i-1] for i in range(1, len(accuracies))]
    sim_improvements = [similarities[i] - similarities[i-1] for i in range(1, len(similarities))]
    
    x_pos = np.arange(len(acc_improvements))
    width = 0.35
    
    axes[1, 1].bar(x_pos - width/2, acc_improvements, width, label='Accuracy Î”', alpha=0.7)
    axes[1, 1].bar(x_pos + width/2, sim_improvements, width, label='Similarity Î”', alpha=0.7)
    axes[1, 1].set_title('Stage-to-Stage Improvements')
    axes[1, 1].set_xlabel('Stage Transition')
    axes[1, 1].set_ylabel('Improvement')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels([f'{stage_names[i]} â†’ {stage_names[i+1]}' for i in range(len(acc_improvements))], 
                              rotation=45, ha='right')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, "multi_stage_comparison.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


# ========== æ•°æ®é›†ç±»å®šä¹‰ ==========
class DocVQADataset(Dataset):
    """
    DocVQAæ•°æ®é›†å°è£…ç±»
    """
    def __init__(self, split_data):
        self.data = split_data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        example = self.data[idx]
        question = "<DocVQA>" + example["question"]
        answer = example["answers"]
        image = example["image"]
        
        # ç¡®ä¿å›¾åƒä¸ºRGBæ ¼å¼
        if image.mode != "RGB":
            image = image.convert("RGB")
        
        return question, answer, image


# ========== ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == "__main__":
    print("ğŸš€ Starting Florence DocVQA Multi-Stage Fine-tuning Pipeline")
    print("=" * 80)
    
    # ========== é…ç½®å‚æ•° ==========
    model_path = "/seu_nvme/home/fenglei/213240634/Florence/Model/Florence"
    dataset_path = "/seu_nvme/home/fenglei/213240634/Florence/dataset_20250615144366/dataset"
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # ä½¿ç”¨cuda:0
    
    # è®­ç»ƒè¶…å‚æ•°
    batch_size = 6
    epochs = 8  # æ¯ä¸ªé˜¶æ®µçš„è®­ç»ƒè½®æ•°
    learning_rate = 1e-6
    num_samples_visualize = 100  # å¯è§†åŒ–æ ·æœ¬æ•°é‡
    
    print(f"ğŸ“± Device: {device}")
    print(f"ğŸ“Š Batch size: {batch_size}")
    print(f"ğŸ”„ Epochs per stage: {epochs}")
    print(f"ğŸ“š Learning rate: {learning_rate}")
    print(f"ğŸ¨ Visualization samples: {num_samples_visualize}")
    
    # ========== åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ ==========
    print("\nğŸ“¥ Loading model and processor...")
    try:
        processor = AutoProcessor.from_pretrained(
            model_path, 
            trust_remote_code=True, 
            revision='refs/pr/6'
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            trust_remote_code=True, 
            revision='refs/pr/6'
        ).to(device)
        print("âœ… Model and processor loaded successfully")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        exit(1)
    
    # ========== åŠ è½½æ•°æ®é›† ==========
    print("\nğŸ“‚ Loading dataset...")
    try:
        data = load_dataset(dataset_path)
        train_dataset = DocVQADataset(data["train"])
        val_dataset = DocVQADataset(data["validation"])
        print(f"âœ… Dataset loaded successfully")
        print(f"   Training samples: {len(train_dataset)}")
        print(f"   Validation samples: {len(val_dataset)}")
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        exit(1)
    
    # ========== æ•°æ®æ•´ç†å™¨ ==========
    def collate_fn(batch):
        questions, answers, images = zip(*batch)
        inputs = processor(
            text=list(questions), 
            images=list(images), 
            return_tensors="pt", 
            padding=True
        ).to(device)
        return inputs, answers
    
    # ========== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ==========
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=True,
        num_workers=0,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        num_workers=0,
    )
    
    print(f"âœ… Data loaders created")
    print(f"   Training batches: {len(train_loader)}")
    print(f"   Validation batches: {len(val_loader)}")
    
    # ========== å¼€å§‹å¤šé˜¶æ®µè®­ç»ƒ ==========
    print("\nğŸ‹ï¸ Starting multi-stage training pipeline...")
    
    try:
        results = run_all_training_stages(
            train_loader=train_loader,
            val_loader=val_loader,
            model=model,
            processor=processor,
            device=device,
            epochs=epochs,
            lr=learning_rate,
            num_samples=num_samples_visualize
        )
        
        # ========== æ‰“å°æœ€ç»ˆç»“æœ ==========
        print("\n" + "=" * 80)
        print("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        print("\nğŸ“Š Final Results Summary:")
        print("-" * 40)
        
        for stage, metrics in results.items():
            print(f"Stage: {stage}")
            print(f"  Vision Trainable: {metrics['vision_trainable_ratio']:.0%}")
            print(f"  Accuracy: {metrics['accuracy']:.4f}")
            print(f"  Similarity: {metrics['similarity']:.4f}")
            print(f"  Time: {metrics['stage_time']:.2f}s")
            print("-" * 40)
        
        print("\nğŸ“ Files Generated:")
        print("  - Training logs: train_log_*.txt")
        print("  - Visualization: visualization_*/")
        print("  - Final results: final_results_all_stages.json")
        print("  - Comparison charts: multi_stage_comparison.png")
        
        print("\nâœ… All training stages completed successfully!")
        
    except Exception as e:
        print(f"âŒ Error during training pipeline: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
